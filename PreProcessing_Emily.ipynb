{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types, Row, Column\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"finalProject\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in raw text set and write to parquet\n",
    "#train = spark.read.option('header', 'false').csv('gs:/notebooks/train.txt', sep='\\t')\n",
    "#train.write.format('parquet').save('data/train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODULO_NUMBER = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in parqet\n",
    "train = spark.read.parquet('data/train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename label column\n",
    "train = train.withColumnRenamed('_c0', 'label')\n",
    "\n",
    "# remove underscores\n",
    "for c in train.columns[1:]:\n",
    "    train = train.withColumnRenamed(c, c.strip('_'))\n",
    "\n",
    "for i,c in enumerate(train.columns[1:14]):\n",
    "    newName = 'n' + str(i)\n",
    "    train = train.withColumnRenamed(c, newName)\n",
    "    \n",
    "for i,c in enumerate(train.columns[14:]):\n",
    "    newName = 'c' + str(i)\n",
    "    train = train.withColumnRenamed(c, newName)\n",
    "\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast numerical is float\n",
    "for c in train.columns[:14]:\n",
    "    train = train.withColumn(c, train[c].cast('float'))\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab a sample\n",
    "s = train.sample(False, 0.001)\n",
    "#s = train\n",
    "s.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on sample\n",
    "trainSample, testSample = s.randomSplit([9.0, 1.0], 666)\n",
    "trainSample = trainSample.cache()\n",
    "testSample = testSample.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSample.count(), testSample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on full\n",
    "train, test = train.randomSplit([9.0, 1.0], 666)\n",
    "train = train.cache()\n",
    "test = test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeNumeric(trainDf, testDf):\n",
    "    stats = trainDf[trainDf.columns[1:14]].describe()\n",
    "    maxes = np.array(stats[stats['summary'] == 'max'].collect())[0][1:]\n",
    "    mins = np.array(stats[stats['summary'] == 'min'].collect())[0][1:]\n",
    "    maxes = [float(m) for m in maxes]\n",
    "    mins = [float(m) for m in mins]\n",
    "    \n",
    "    for i,c in enumerate(trainDf.columns[1:14]):\n",
    "        trainDf = trainDf.withColumn(c, (trainDf[c] - mins[i]) / (maxes[i] - mins[i]))\n",
    "        testDf = testDf.withColumn(c, (testDf[c] - mins[i]) / (maxes[i] - mins[i]))\n",
    "        \n",
    "    # NEED TO FIGURE THIS OUT FIRST\n",
    "    trainDf = trainDf.na.fill(0, subset=trainDf.columns[1:14])\n",
    "    testDf = testDf.na.fill(0, subset=testDf.columns[1:14])\n",
    "    \n",
    "    return trainDf.cache(), testDf.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## on sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on sample\n",
    "trainSample, testSample = normalizeNumeric(trainSample, testSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## on full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "train, test = normalizeNumeric(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeatureVector2(trainDf, testDf):\n",
    "    \n",
    "    \n",
    "    def findInfrequentValues(c, n=10):\n",
    "        # c is the column that we are operating on\n",
    "        counts = trainDf.groupBy(c).count()\n",
    "        infrequentValues = counts.filter(counts['count'] <= n)\n",
    "        s = infrequentValues.agg(F.collect_set(c)).collect()[0][0]\n",
    "        return s\n",
    "    \n",
    "    def replaceInfrequentValues(row_value):\n",
    "        if row_value in infreq_values:\n",
    "            return \"infreq\"\n",
    "        else:\n",
    "            return str(row_value)\n",
    "        \n",
    "    #def replaceInfreqWrapper(infreq_list):\n",
    "    #    return F.udf(lambda l: replaceInfrequentValues(l, infreq_list))\n",
    "    \n",
    "    #replaceInfreqWrapper=F.udf(lambda x: replaceInfrequentValues(x, infreq_values), types.StringType())\n",
    "        \n",
    "    replace_infreq_udf = F.udf(replaceInfrequentValues)\n",
    "\n",
    "    # create hash function for binning categorical variables\n",
    "    def hashValues(row):\n",
    "        if row != None:\n",
    "            # return integer value of hex label, modulo by 10000 (keep only the last 4 digits)\n",
    "            return str(int('0x' + row, 16) % 100000)\n",
    "        else:\n",
    "            return str(row)\n",
    "    \n",
    "    # create the udf object from the helper function\n",
    "    hash_udf = F.udf(hashValues)\n",
    "    \n",
    "    # hash all hex strings in both train and test\n",
    "    for c in trainDf.columns[14:]:\n",
    "        infreq_values = findInfrequentValues(trainDf[c])\n",
    "        trainDf = trainDf.withColumn(c, replace_infreq_udf(trainDf[c]))\n",
    "        testDf = testDf.withColumn(c, replace_infreq_udf(testDf[c]))\n",
    "    return trainDf.cache(), testDf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeatureVector(trainDf, testDf):\n",
    "    \n",
    "    \n",
    "    def findInfrequentValues(c, n=10):\n",
    "        # c is the column that we are operating on\n",
    "        counts = trainDf.groupBy(c).count()\n",
    "        infrequentValues = counts.filter(counts['count'] <= n)\n",
    "        s = infrequentValues.agg(F.collect_set(c)).collect()[0][0]\n",
    "        return s\n",
    "    \n",
    "    # this value infreq_values is GLOBAL, rather than a param passed into the function\n",
    "    # i really don't love that, but I can't figure out how to do it otherwise\n",
    "    def replaceInfrequentValues(row_value):\n",
    "        if row_value in infreq_values:\n",
    "            return \"infreq\"\n",
    "        else:\n",
    "            return row_value\n",
    "        \n",
    "    #def replaceInfreqWrapper(infreq_list):\n",
    "    #    return F.udf(lambda l: replaceInfrequentValues(l, infreq_list))\n",
    "    \n",
    "    #replaceInfreqWrapper=F.udf(lambda x: replaceInfrequentValues(x, infreq_values), types.StringType())\n",
    "        \n",
    "    replace_infreq_udf = F.udf(replaceInfrequentValues)\n",
    "\n",
    "    # create hash function for binning categorical variables\n",
    "    def hashValues(row):\n",
    "        if row == \"infreq\":\n",
    "            return str(row)\n",
    "        elif row != None:\n",
    "            # return integer value of hex label, modulo by 10000 (keep only the last 4 digits)\n",
    "            return str(int('0x' + row, 16) % 100000)\n",
    "        else:\n",
    "            return str(row)\n",
    "    \n",
    "    # create the udf object from the helper function\n",
    "    hash_udf = F.udf(hashValues)\n",
    "    \n",
    "    # hash all hex strings in both train and test\n",
    "    for c in trainDf.columns[14:]:\n",
    "        infreq_values = findInfrequentValues(trainDf[c])\n",
    "        trainDf = trainDf.withColumn(c, replace_infreq_udf(trainDf[c]))\n",
    "        testDf = testDf.withColumn(c, replace_infreq_udf(testDf[c]))\n",
    "        unique_values = trainDf.agg(F.countDistinct(trainDf[c]))\n",
    "        #if unique_values > 100000:\n",
    "        #    trainDf = trainDf.withColumn(c, hash_udf(trainDf[c]))\n",
    "        #    testDf = testDf.withColumn(c, hash_udf(testDf[c]))\n",
    "        \n",
    "    # index the hash values into categories\n",
    "    for c in trainDf.columns[14:]:\n",
    "        newCol = c + '_idx'\n",
    "        indexer = StringIndexer(inputCol=c, outputCol=newCol, handleInvalid='keep')\n",
    "        f = indexer.fit(trainDf)\n",
    "        trainDf = f.transform(trainDf)\n",
    "        testDf = f.transform(testDf)\n",
    "        \n",
    "    # One-hot encode the categorical indices\n",
    "    inputCols = trainDf.columns[40:]\n",
    "    outputCols = [c.strip('_idx') + '_OHE' for c in inputCols]\n",
    "    encoder = OneHotEncoderEstimator(inputCols=inputCols, outputCols=outputCols)\n",
    "    e = encoder.fit(trainDf)\n",
    "    trainDf = e.transform(trainDf)\n",
    "    testDf = e.transform(testDf)\n",
    "    \n",
    "    # assemble all features into single SparseVector column\n",
    "    cols = [c for c in trainDf.columns if 'n' in c or 'OHE' in c]\n",
    "    v = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "    trainDf = v.transform(trainDf)\n",
    "    testDf = v.transform(testDf)\n",
    "    \n",
    "    return unique_values.cache(), trainDf.cache(), testDf.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## on sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on sample\n",
    "uv, trainSample, testSample = createFeatureVector(trainSample, testSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSample.columns[-1], testSample.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSample.select('features').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSample.select('features').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSample.write.format('parquet').save('gs:/notebooks/data/fullTrainMod100k.parquet')\n",
    "testSample.write.format('parquet').save('gs:/notebooks/data/fullTestMod100k.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## on full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# on full\n",
    "out1, out2 = createFeatureVector2(trainSample, testSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = createFeatureVector(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select('features').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.select('features').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.write.format('parquet').save('gs:/notebooks/data/fullTrainMod100k.parquet')\n",
    "test.write.format('parquet').save('gs:/notebooks/data/fullTestMod100k.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashCategoricals(trainDf, testDf):\n",
    "    # create hash function for binning categorical variables\n",
    "    def hashValues(row):\n",
    "        if row != None:\n",
    "            # return integer value of hex label, modulo by 10000 (keep only the last 4 digits)\n",
    "            return str(int('0x' + row, 16) % 10000)\n",
    "        else:\n",
    "            return str(row)\n",
    "    \n",
    "    # create the udf object from the helper function\n",
    "    udf_object = F.udf(hashValues)\n",
    "    \n",
    "    # hash all hex strings in both train and test\n",
    "    for c in trainDf.columns[14:]:\n",
    "        trainDf = trainDf.withColumn(c, udf_object(trainDf[c]))\n",
    "        testDf = testDf.withColumn(c, udf_object(testDf[c]))\n",
    "    \n",
    "    return trainDf.cache(), testDf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSample, testSample = hashCategoricals(trainSample, testSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline implementation\n",
    "def createFeatureVector2(trainDf, testDf):\n",
    "    # generate stages for pipeline\n",
    "    stages = []\n",
    "    \n",
    "    # create indexer to hash values into categories\n",
    "    for c in trainDf.columns[14:]:\n",
    "        strIdxCol = c + '_idx'\n",
    "        oheCol = strIdxCol.strip('_idx') + '_OHE'\n",
    "        indexer = StringIndexer(inputCol=c, outputCol=strIdxCol, handleInvalid='keep')\n",
    "        OHE = OneHotEncoderEstimator(inputCols=strIdxCol, outputCols=oheCol, dropLast=False)\n",
    "        stages += [indexer, OHE]\n",
    "        \n",
    "    # One-hot encode the categorical indices\n",
    "#     inputCols = trainDf.columns[40:]\n",
    "#     outputCols = [c.strip('_idx') + '_OHE' for c in inputCols]\n",
    "#     encoder = OneHotEncoderEstimator(inputCols=inputCols, outputCols=outputCols, dropLast=False)\n",
    "#     stages += [encoder]\n",
    "#     print(stages)\n",
    "#     e = encoder.fit(trainDf)\n",
    "#     trainDf = e.transform(trainDf)\n",
    "#     testDf = e.transform(testDf)\n",
    "    \n",
    "    # assemble all features into single SparseVector column\n",
    "#     cols = [c for c in trainDf.columns if 'n' in c or 'OHE' in c]\n",
    "#     v = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "#     stages += [v]\n",
    "#     trainDf = v.transform(trainDf)\n",
    "#     testDf = v.transform(testDf)\n",
    "    \n",
    "    pipe = Pipeline(stages=stages)\n",
    "    model = pipe.fit(trainDf)\n",
    "    trainDf = model.transform(trainDf)\n",
    "    testDf = model.transform(testDf)\n",
    "    \n",
    "    return trainDf.cache(), testDf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer, tester = createFeatureVector2(trainSample, testSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.columns[-1], tester.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSample.write.format('parquet').save('data/trainSample.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}