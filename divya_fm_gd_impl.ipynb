{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "app_name = \"final_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 10000 data points for testing the gd \n",
    "!head -n 10000 dac/sample_train.txt > dac/sample_train_10000.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization machines using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rdd = sc.textFile('dac/sample_train_10000.txt').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize learning rate, number of latent factors, mean, standard deviation\n",
    "learning_rate = 0.01\n",
    "num_latent_factors = 10\n",
    "# used for initializing the parameters for factorized interactions\n",
    "mean = 0\n",
    "# used for initializing the parameters for factorized interactions\n",
    "std = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the regularization parameters\n",
    "# techincally these should come from cross validation\n",
    "reg_bias = 0.01\n",
    "reg_independent = 0.01\n",
    "reg_interaction = np.full(num_latent_factors, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_calculation(features, w0, W, V):\n",
    "    # dot product of independent features and weights\n",
    "    # appending the bias term as well\n",
    "    independent_features = np.append([1.0], features)\n",
    "    independent_weights = np.append([w0], W)\n",
    "    \n",
    "    # get combinations of features for the interaction terms with degree 2\n",
    "    total_interaction = 0.0\n",
    "    for subset in itertools.combinations(enumerate(features),2):\n",
    "        index1 = int(subset[0][0])\n",
    "        index2 = int(subset[1][0])\n",
    "        feature1 = float(subset[0][1])\n",
    "        feature2 = float(subset[1][1])\n",
    "        # dot product of v parameters and features\n",
    "        total_interaction += np.dot(V[index1],V[index2]) * feature1 * feature2\n",
    "    y_hat = np.dot(independent_features,independent_weights) + total_interaction\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w0_gradients(data):\n",
    "    y = data[0]\n",
    "    features = data[1:]\n",
    "    y_hat = prediction_calculation(features, w0_b.value, W_b.value, V_b.value)\n",
    "    gradient = ((1.0/(1.0+math.exp(-y*y_hat))) - 1)*y\n",
    "    yield gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_W_gradients(data):\n",
    "    y = data[0]\n",
    "    features = data[1:]\n",
    "    y_hat = prediction_calculation(features, w0_b.value, W_b.value, V_b.value)\n",
    "    gradient = ((1.0/(1.0+math.exp(-y*y_hat))) - 1)*y*features\n",
    "    yield gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_V_gradients(data):\n",
    "    y = data[0]\n",
    "    features = data[1:]\n",
    "    len_features = len(features)\n",
    "    V = V_b.value\n",
    "    y_hat = prediction_calculation(features, w0_b.value, W_b.value, V)\n",
    "    # calculating the sum part of the the differenciation_interaction\n",
    "    sum_v_features = np.empty((len_features,num_latent_factors_b.value))\n",
    "    # executing xl*sum(vj,f * xj) where j!= l (for all j features except l)\n",
    "    for i, feature in enumerate(features):\n",
    "        # handle the first and last elements\n",
    "        if i == 0:\n",
    "            sum_v_features[i] = np.dot(features[i+1:],V[i+1:,:])\n",
    "            continue\n",
    "        if i == len_features-1:\n",
    "            sum_v_features[i] = np.dot(features[:i],V[:i,:])\n",
    "            continue\n",
    "        sum_v_features[i] = np.dot(np.concatenate((features[:i], features[i+1:])),np.concatenate((V[:i,:], V[i+1:,:])))\n",
    "    # reshape the feature vector to be able to multiple each feature with it's sum of (vj,f*xj) to compute the expression\n",
    "    # xl*sum(vj,f * xj) where j!= l (for all j features except l)\n",
    "    feature_reshape = np.reshape(features, (len_features, 1))\n",
    "    # now multiply to get the entire V at once\n",
    "    differenciation_interaction = np.multiply(feature_reshape,sum_v_features)\n",
    "    gradient = ((1.0/(1.0+math.exp(-y*y_hat))) - 1)*y*differenciation_interaction\n",
    "    yield gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(line):\n",
    "    data = np.array(line.split('\\t'), dtype = 'float')\n",
    "    yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_gd(data_rdd, learning_rate, num_latent_factors, mean, std, reg_bias, reg_independent, reg_interaction, num_steps):    \n",
    "    # -1 to not count the output variable\n",
    "    num_features = len(data_rdd.take(1)[0].split('\\t'))-1 \n",
    "    \n",
    "    # initialize the model parameters\n",
    "    w0 = 0.0\n",
    "    W = np.zeros(num_features)\n",
    "    V = np.random.normal(mean, std, (num_features,num_latent_factors))\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        # broadcast the model parameters\n",
    "        w0_b = sc.broadcast(w0)\n",
    "        W_b = sc.broadcast(W)\n",
    "        V_b = sc.broadcast(V)\n",
    "        num_latent_factors_b = sc.broadcast(num_latent_factors)\n",
    "        \n",
    "        # split and create array\n",
    "        parse_data_rdd = data_rdd.flatMap(parse).cache() # map or flat map?\n",
    "        \n",
    "        # perform w0 updates\n",
    "        w0_gradient = parse_data_rdd.flatMap(get_w0_gradients).mean()\n",
    "        w0 -= learning_rate * w0_gradient + 2 * reg_bias.value * w0\n",
    "        \n",
    "        # perform W updates\n",
    "        W_gradient = parse_data_rdd.flatMap(get_W_gradients).mean()\n",
    "        W -= learning_rate * W_gradient + 2 * reg_independent.value * W\n",
    "        \n",
    "        # perform V updates\n",
    "        V_gradient = parse_data_rdd.flatMap(get_V_gradients).mean()\n",
    "        V -= learning_rate * V_gradient + 2 * np.multiply(reg_interaction.value, V)\n",
    "        \n",
    "    return w0, W, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, W, V = fm_gd(sample_rdd, learning_rate, num_latent_factors, mean, std, reg_bias, reg_independent, reg_interaction, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dataset of just few rows and columns to test the implementation\n",
    "test_rdd = sc.textFile('dac/sample_test_impl.txt').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize learning rate, number of latent factors, mean, standard deviation\n",
    "learning_rate = 0.01\n",
    "num_latent_factors = 2\n",
    "# used for initializing the parameters for factorized interactions\n",
    "mean = 0\n",
    "# used for initializing the parameters for factorized interactions\n",
    "std = 0.01\n",
    "reg_bias = 0.01\n",
    "reg_independent = 0.01\n",
    "reg_interaction = np.full(num_latent_factors, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(test_rdd.take(1)[0].split('\\t'))-1 \n",
    "# initialize the model parameters\n",
    "w0 = 0.0\n",
    "W = np.zeros(num_features)\n",
    "V = np.random.normal(mean, std, (num_features,num_latent_factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.16864343e-03,  9.91140812e-03],\n",
       "       [-1.47871127e-02, -5.82742997e-03],\n",
       "       [-3.38995550e-03, -8.70194226e-05],\n",
       "       [ 7.25278359e-04, -1.04087130e-02]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.dot(V[0],V[1])*1 + np.dot(V[0],V[2])*5 + np.dot(V[0],V[3])*0 + np.dot(V[1],V[2])*5 + np.dot(V[1],V[3])*0 + np.dot(V[2],V[3])*0\n",
    "x2 = np.dot(V[0],V[1])*6 + np.dot(V[0],V[2])*8 + np.dot(V[0],V[3])*2 + np.dot(V[1],V[2])*12 + np.dot(V[1],V[3])*3 + np.dot(V[2],V[3])*4\n",
    "x3 = np.dot(V[0],V[1])*40 + np.dot(V[0],V[2])*44 + np.dot(V[0],V[3])*4 + np.dot(V[1],V[2])*110 + np.dot(V[1],V[3])*10 + np.dot(V[2],V[3])*11\n",
    "x4 = np.dot(V[0],V[1])*1 + np.dot(V[0],V[2])*5 + np.dot(V[0],V[3])*0 + np.dot(V[1],V[2])*5 + np.dot(V[1],V[3])*0 + np.dot(V[2],V[3])*0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient1= ((1.0/(1.0+math.exp(-0*x1))) - 1)*0\n",
    "gradient2= ((1.0/(1.0+math.exp(-1*x2))) - 1)*1\n",
    "gradient3= ((1.0/(1.0+math.exp(-0*x3))) - 1)*0\n",
    "gradient4= ((1.0/(1.0+math.exp(-1*x4))) - 1)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0, -0.4998329300991783, -0.0, -0.49991914922926506]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[gradient1,gradient2,gradient3,gradient4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.24993801983211084"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([gradient1,gradient2,gradient3,gradient4])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_interaction = 0.0\n",
    "for subset in itertools.combinations(enumerate([1,1,5,0]),2):\n",
    "    index1 = subset[0][0]\n",
    "    index2 = subset[1][0]\n",
    "    feature1 = subset[0][1]\n",
    "    feature2 = subset[1][1]\n",
    "    # dot product of v parameters and features\n",
    "    total_interaction += np.dot(V[index1],V[index2]) * feature1 * feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003234030857583696"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast the model parameters\n",
    "w0_b = sc.broadcast(w0)\n",
    "W_b = sc.broadcast(W)\n",
    "V_b = sc.broadcast(V)\n",
    "num_latent_factors_b = sc.broadcast(num_latent_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(line):\n",
    "    data = line.split('\\t')\n",
    "    y = data[0]\n",
    "    features = data[1:]\n",
    "    yield features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.24993801983211084"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0_gradient = test_rdd.flatMap(parse).flatMap(get_w0_gradients).mean()\n",
    "w0_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient1w= ((1.0/(1.0+math.exp(-0*x1))) - 1)*0*np.array([1,1,5,0])\n",
    "gradient2w= ((1.0/(1.0+math.exp(-1*x2))) - 1)*1*np.array([2,3,4,1])\n",
    "gradient3w= ((1.0/(1.0+math.exp(-0*x3))) - 1)*0*np.array([4,10,11,1])\n",
    "gradient4w= ((1.0/(1.0+math.exp(-1*x4))) - 1)*1*np.array([1,1,5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.37489625, -0.49985448, -1.12473187, -0.12495823])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([gradient1w,gradient2w,gradient3w,gradient4w])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.37489625, -0.49985448, -1.12473187, -0.12495823])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_gradient = test_rdd.flatMap(parse).flatMap(get_W_gradients).mean()\n",
    "w_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_1 = 2 * (np.multiply(V[1],3) + np.multiply(V[2],4) + np.multiply(V[3],1))\n",
    "grad_2 = 3 * (np.multiply(V[0],2) + np.multiply(V[2],4) + np.multiply(V[3],1))\n",
    "grad_3 = 4 * (np.multiply(V[1],3) + np.multiply(V[0],2) + np.multiply(V[3],1))\n",
    "grad_4 = 1 * (np.multiply(V[1],3) + np.multiply(V[2],4) + np.multiply(V[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11439176, -0.05647816],\n",
       "       [-0.06351549,  0.02719808],\n",
       "       [-0.20789339, -0.03227275],\n",
       "       [-0.06625845,  0.00199245]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([grad_1,grad_2,grad_3,grad_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient2V= ((1.0/(1.0+math.exp(-1*x2))) - 1)*1*np.array([grad_1,grad_2,grad_3,grad_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05717677,  0.02822964],\n",
       "       [ 0.03174713, -0.01359449],\n",
       "       [ 0.10391196,  0.01613098],\n",
       "       [ 0.03311815, -0.00099589]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_1 = (np.multiply(V[1],3) + np.multiply(V[2],4) + np.multiply(V[3],1))\n",
    "grad_2 = (np.multiply(V[0],2) + np.multiply(V[2],4) + np.multiply(V[3],1))\n",
    "grad_3 = (np.multiply(V[1],3) + np.multiply(V[0],2) + np.multiply(V[3],1))\n",
    "grad_4 = (np.multiply(V[1],3) + np.multiply(V[2],4) + np.multiply(V[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05719588, -0.02823908],\n",
       "       [-0.02117183,  0.00906603],\n",
       "       [-0.05197335, -0.00806819],\n",
       "       [-0.06625845,  0.00199245]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([grad_1,grad_2,grad_3,grad_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05719588, -0.02823908, -0.02117183,  0.00906603])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((grad_1, grad_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_V(data):\n",
    "    y = data[0]\n",
    "    features = data[1:]\n",
    "    len_features = len(features)\n",
    "    V = V_b.value\n",
    "    y_hat = prediction_calculation(features, w0_b.value, W_b.value, V)\n",
    "    # calculating the sum part of the the differenciation_interaction\n",
    "    sum_v_features = np.zeros((len_features,num_latent_factors_b.value))\n",
    "    for i, feature in enumerate(features):\n",
    "        if i == 0:\n",
    "            sum_v_features[i] = np.dot(features[i+1:],V[i+1:,:]) # check this :\n",
    "            continue\n",
    "        if i == len_features -1:\n",
    "            sum_v_features[i] = np.dot(features[:i],V[:i,:]) # check this :\n",
    "            continue\n",
    "        sum_v_features[i] = np.dot(np.concatenate((features[:i], features[i+1:])),np.concatenate((V[:i,:], V[i+1:,:])))\n",
    "    yield sum_v_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  0.],\n",
       "        [ 0., -0.],\n",
       "        [ 0., -0.],\n",
       "        [ 0., -0.]]), array([[ 0.05717677,  0.02822964],\n",
       "        [ 0.03174713, -0.01359449],\n",
       "        [ 0.10391196,  0.01613098],\n",
       "        [ 0.03311815, -0.00099589]]), array([[ 0.,  0.],\n",
       "        [ 0., -0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.]]), array([[ 0.01586588,  0.00313076],\n",
       "        [ 0.0105575 , -0.00473739],\n",
       "        [ 0.04738173, -0.01020829],\n",
       "        [ 0.        , -0.        ]])]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_gradient = test_rdd.flatMap(parse).flatMap(get_V_gradients).collect()\n",
    "v_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing array manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 1 2\n",
      "0 2 1 3\n",
      "1 2 2 3\n"
     ]
    }
   ],
   "source": [
    "for subset in itertools.combinations(enumerate([1,2,3]),2):\n",
    "    print(subset[0][0],subset[1][0],subset[0][1],subset[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9, 7, 9]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot([[1,2,3]],[[1,1,1],[1,0,1],[2,2,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [2, 0, 2],\n",
       "       [6, 6, 6]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply([[1],[2],[3]],[[1,1,1],[1,0,1],[2,2,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape([1,2,3], (3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [2, 0, 2],\n",
       "       [6, 6, 6]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(np.reshape([1,2,3], (3, 1)),[[1,1,1],[1,0,1],[2,2,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [1, 0, 3],\n",
       "       [2, 4, 6]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.multiply([1,2,3],[[1,1,1],[1,0,1],[2,2,2]])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 6])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(2,[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  4.,  6.],\n",
       "       [ 2.,  0.,  6.],\n",
       "       [ 4.,  8., 12.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.0 * test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 3],\n",
       "       [2, 4, 6]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some experimenting with data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = spark.read.option('header', 'false').csv('dac/sample_train_10000.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data into parquet file\n",
    "sample_df.write.format('parquet').save('dac/sample_train_10000.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_parquet_df = spark.read.parquet('dac/sample_train_10000.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='0', _c1='1', _c2='1', _c3='5', _c4='0', _c5='1382', _c6='4', _c7='15', _c8='2', _c9='181', _c10='1', _c11='2', _c12=None, _c13='2', _c14='68fd1e64', _c15='80e26c9b', _c16='fb936136', _c17='7b4723c4', _c18='25c83c98', _c19='7e0ccccf', _c20='de7995b8', _c21='1f89b562', _c22='a73ee510', _c23='a8cd5504', _c24='b2cb9c98', _c25='37c9c164', _c26='2824a5f6', _c27='1adce6ef', _c28='8ba8b39a', _c29='891b62e7', _c30='e5ba7672', _c31='f54016b9', _c32='21ddcdc9', _c33='b1252a9d', _c34='07b5194c', _c35=None, _c36='3a171ecb', _c37='c5c50484', _c38='e8b83407', _c39='9727dd16')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_parquet_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_df = sample_parquet_df.select((sample_parquet_df._c1 + 10).alias('w0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c1='1', _c2='1', _c3='5', _c4='0', _c5='1382', _c6='4', _c7='15', _c8='2', _c9='181')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_parquet_df.select(sample_parquet_df.columns[1:10]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    sample_parquet_df.select((sample_parquet_df._c1 + 10).alias('w0'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
